# The amount of coturn instances to run.  NOTE: Only one coturn can run per node due
# to `hostNetwork`. If this number is higher than the amount of nodes that can
# be used for scheduling (Also see `nodeSelector`) pods will remain in a
# pending state untill you add more capacity.
replicaCount: 1

image:
  repository: quay.io/wire/coturn
  pullPolicy: IfNotPresent
  # overwrite the tag here, otherwise `appVersion` of the chart will be used
  tag: ""

# If you have multiple deployments of coturn running in one cluster, it is
# important that they run on disjoint sets of nodes, you can use nodeSelector to enforce this
nodeSelector: {}

podSecurityContext:
  fsGroup: 31338

securityContext:
  # Pick a high number that is unlikely to conflict with the host
  # https://kubesec.io/basics/containers-securitycontext-runasuser/
  runAsUser: 31338

coturnTurnListenPort: 3478
coturnMetricsListenPort: 9641
coturnTurnTlsListenPort: 5349

tls:
  enabled: false
  secretRef:
  reloaderImage:
    # container image containing https://github.com/Pluies/config-reloader-sidecar
    # for handling runtime certificate reloads.
    repository: quay.io/wire/config-reloader-sidecar
    pullPolicy: IfNotPresent
    tag: 1aa6cbbf2ce3a5182ec47e3579bbcb8f47e22fdc

metrics:
  serviceMonitor:
    enabled: false

# This chart optionally supports waiting for traffic to drain from coturn
# before pods are terminated. Warning: coturn does not have any way to steer
# incoming client traffic away from itself on its own, so this functionality
# relies on external traffic management (e.g. service discovery for active coturn
# instances) to prevent clients from sending new requests to pods which are in a
# terminating state.
coturnGracefulTermination: false
# Grace period for terminating coturn pods, after which they will be forcibly
# terminated. This setting is only effective when coturnGracefulTermination is
# set to true.
coturnGracePeriodSeconds: 86400 # one day
