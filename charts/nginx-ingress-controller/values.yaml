# Default values for nginx-ingress-controller

nginx-ingress:
  controller:
    config:
      # NOTE: These are some sane defaults (compliant to TR-02102-2), you may want to overrride them on your own installation
      # For TR-02102-2 see https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Publications/TechGuidelines/TG02102/BSI-TR-02102-2.html
      # As a Wire employee, for Wire-internal discussions and context see
      # * https://wearezeta.atlassian.net/browse/FS-33
      # * https://wearezeta.atlassian.net/browse/FS-444
      #
      # Note/FUTUREWORK: this current ingress-controller does not yet support TLS 1.3 (and its ciphers). An upgrade/different helm chart will be provided in the future.
      ssl-ciphers: "ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384"
      http2-max-field-size: 16k
      http2-max-header-size: 32k
      proxy-buffer-size: 16k
      proxy-body-size: 1024m

      # custom log format, remove query parameters from logs as they sometimes contain sensitive information like access tokens (context: websocket establishment in browsers)
      # See also SEC-47 for context.
      # default log format (for image 0.30.0) in
      # https://github.com/kubernetes/ingress-nginx/blob/49f20f849cc13564691acc49f639955f02f3c75e/docs/user-guide/nginx-configuration/configmap.md
      # If ever needing to debug query parameter usage, you can use the (sanitized) logs from nginz instead.
      log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request_method $uri" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id'
# Normally, NodePort will listen to traffic on all nodes, and uses kube-proxy
# to redirect to the node that actually runs nginx-ingress-controller. However
# one problem with this is that this traffic is NAT'ed. This means that nginx
# will not have access to the source IP address from which the request
# originated.  We want to have this source IP address for potentially logging
# and rate-limiting based on it. By setting externalTrafficPolicy: local,
# nodes will no longer forward requests to other nodes if they receive a
# request that they themselves can not handle. Upside is that the traffic is
# now not NAT'ed anymore, and we get access to the source IP address. Downside
# is that you need to know beforehand which nodes run a certain pod. However,
# with kubernetes a pod can be rescheduled to any node at any time so we can
# not trust this.  We could do something with node affinities to decide apriori
# on what set of nodes will be publicly reachable and make sure the nginx
# controller pods are only ran on there but for now that sounds a bit overkill.
# Instead, we just simply run the ingress controller on each node using a
# daemonset. This means that any node in the cluster can receive requests and
# redirect them to the correct service, whilst maintaining the source ip
# address.  The ingress controller is sort of taking over the role of what
# kube-proxy was doing before.
# More information:
#   https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typenodeport
#   https://kubernetes.github.io/ingress-nginx/deploy/baremetal/
#
# There are also downsides to setting externalTrafficPolicy: Local
# Please look at the following blog post, which very clearly explains the upsides and
# downsides of this setting
#   https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies
    kind: DaemonSet
    ingressClass: nginx
    # By default, each node will now be configured to accept ingress traffic. You should add
    # all the nodes to your external load balancer, or add them to DNS records.
    #
    # You can also decide to only run the nginx controller on a subset of workers
    # nodes in your cluster. This is especially useful in large clusters, where you
    # probably don't want all workers nodes reachable from the internet or behind
    # your company's loadbalancer.  Instead you probably have a subset of nodes
    # that are able to receive traffic from the outside world.
    #
    # You can set node labels in an ad-hoc way with kubectl:
    # $ kubectl label nodes mynode wire.com/role=ingress
    #
    # Or in ansible you can set the `node_labels` variable in a declarative way
    # https://github.com/kubernetes-sigs/kubespray/blob/master/docs/vars.md#other-service-variables
    #
    # In your inventory file you could for example set:
    #
    # [ingress]
    # mynode1
    # mynode2
    #
    # [ingress:vars]
    # node_labels = {"wire.com/role": "ingress"}
    #
    # If you have labelled the nodes that are fit for receiving ingress, you
    # can uncomment the following nodeSelector to make sure that only those
    # nodes actually have the ingress controller running:
    #
    # nodeSelector:
    #  wire.com/role: ingress
    service:
      # If your kubernetes installation has support for LoadBalancers, set
      # type: LoadBalancer
      #
      # This will then automatically add and remove health / unhealthy nodes
      # from the set of servers that should receive traffic, and if you would
      # add a new node to the cluster, it will automatically be added to the
      # LoadBalancer.  If you set it to NodePort, then you need to manually add
      # the node IP addresses to the external load balancer when you add nodes,
      # and remove them when you remove nodes
      type: NodePort  # or LoadBalancer
      externalTrafficPolicy: Local
      nodePorts:
        # The nginx instance is exposed on ports 31773 (https) and 31772 (http)
        # on the node on which it runs. You should add a port-forwarding rule
        # on the node or on the loadbalancer that forwards ports 443 and 80 to
        # these respective ports. See ansible/iptables.yml how to do this with
        # ansible and Iptables
        https: 31773
        http: 31772
